{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport gc\nimport pickle\n\n%matplotlib inline\n\npd.set_option('display.max_rows', 304)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-28T18:39:38.677115Z","iopub.execute_input":"2022-03-28T18:39:38.677825Z","iopub.status.idle":"2022-03-28T18:39:39.823516Z","shell.execute_reply.started":"2022-03-28T18:39:38.677723Z","shell.execute_reply":"2022-03-28T18:39:39.822643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some notes/ideas from EDA:\n* Looks like missing values were replaced with 0 (including in target!).\n* Feature for number of time_ids since this investment_id started?\n* Submit one version for scoring with lag and time features, and one without\n* Note: if doing lag features, will need to be careful to combine test data with train data in order to make sure lag features are included.\n* All features are scaled already (including target) - roughly mean of 0 and standard deviation 1. However, many features are bimodal, or skewed.\n* All features have low correlation with target, but there are some that correlate with each other.\n* Initial validation: time-based\n* Validation goal: want validation score to be similar to leaderboard score","metadata":{}},{"cell_type":"code","source":"df = pd.read_parquet(\"../input/ubiquant-parquet/train_low_mem.parquet\")\nprint (len(df.index))","metadata":{"execution":{"iopub.status.busy":"2022-03-28T17:39:48.152464Z","iopub.execute_input":"2022-03-28T17:39:48.152763Z","iopub.status.idle":"2022-03-28T17:40:24.832569Z","shell.execute_reply.started":"2022-03-28T17:39:48.152729Z","shell.execute_reply":"2022-03-28T17:40:24.831839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create validation dataset before filtering so that validation data is always the same\n'''\nfeatures = ['f_'+str(i) for i in range(300)]\nval_time_cutoff = np.floor(df['time_id'].quantile(0.75))\nX_val = df.loc[df['time_id']>val_time_cutoff, ['investment_id']+features].values\nY_val = df.loc[df['time_id']>val_time_cutoff, 'target'].values\n\n# save validation data to file\nnp.save('X_val.npy', X_val)\nnp.save('Y_val.npy', Y_val)\n\ndel X_val\ndel Y_val\ngc.collect()\n'''\n\nfeatures = ['f_'+str(i) for i in range(300)]\ninvestment_id = df['investment_id'].copy()\nX = df[['investment_id','time_id']+features].values.astype('float16')\nY = df['target'].values\nnp.save('X.npy', X)\nnp.save('Y.npy', Y)\n\ndel X, Y\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T17:40:24.834257Z","iopub.execute_input":"2022-03-28T17:40:24.834654Z","iopub.status.idle":"2022-03-28T17:40:38.285995Z","shell.execute_reply.started":"2022-03-28T17:40:24.834614Z","shell.execute_reply":"2022-03-28T17:40:38.285151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfeatures = ['f_'+str(i) for i in range(300)]\n\n# segement data for time-based cross-fold validation\ntime_cutoff_50 = np.floor(df['time_id'].quantile(0.5))\ntime_cutoff_60 = np.floor(df['time_id'].quantile(0.6))\ntime_cutoff_70 = np.floor(df['time_id'].quantile(0.7))\ntime_cutoff_80 = np.floor(df['time_id'].quantile(0.8))\ntime_cutoff_90 = np.floor(df['time_id'].quantile(0.9))\n\nX_0_50 = df.loc[df['time_id']<=time_cutoff_50, ['investment_id','time_id']+features].values.astype('float16')\nY_0_50 = df.loc[df['time_id']<=time_cutoff_50, 'target'].values\nnp.save('X_0_50.npy', X_0_50)\nnp.save('Y_0_50.npy', Y_0_50)\ndel X_0_50, Y_0_50\n\nX_50_60 = df.loc[(df['time_id']>time_cutoff_50) & (df['time_id']<=time_cutoff_60), ['investment_id','time_id']+features].values.astype('float16')\nY_50_60 = df.loc[(df['time_id']>time_cutoff_50) & (df['time_id']<=time_cutoff_60), 'target'].values\nnp.save('X_50_60.npy', X_50_60)\nnp.save('Y_50_60.npy', Y_50_60)\ndel X_50_60, Y_50_60\n\nX_60_70 = df.loc[(df['time_id']>time_cutoff_60) & (df['time_id']<=time_cutoff_70), ['investment_id','time_id']+features].values.astype('float16')\nY_60_70 = df.loc[(df['time_id']>time_cutoff_60) & (df['time_id']<=time_cutoff_70), 'target'].values\nnp.save('X_60_70.npy', X_60_70)\nnp.save('Y_60_70.npy', Y_60_70)\ndel X_60_70, Y_60_70\n\nX_70_80 = df.loc[(df['time_id']>time_cutoff_70) & (df['time_id']<=time_cutoff_80), ['investment_id','time_id']+features].values.astype('float16')\nY_70_80 = df.loc[(df['time_id']>time_cutoff_70) & (df['time_id']<=time_cutoff_80), 'target'].values\nnp.save('X_70_80.npy', X_70_80)\nnp.save('Y_70_80.npy', Y_70_80)\ndel X_70_80, Y_70_80\n\nX_80_90 = df.loc[(df['time_id']>time_cutoff_80) & (df['time_id']<=time_cutoff_90), ['investment_id','time_id']+features].values.astype('float16')\nY_80_90 = df.loc[(df['time_id']>time_cutoff_80) & (df['time_id']<=time_cutoff_90), 'target'].values\nnp.save('X_80_90.npy', X_80_90)\nnp.save('Y_80_90.npy', Y_80_90)\ndel X_80_90, Y_80_90\n\nX_90_100 = df.loc[df['time_id']>time_cutoff_90, ['investment_id','time_id']+features].values.astype('float16')\nY_90_100 = df.loc[df['time_id']>time_cutoff_90, 'target'].values\nnp.save('X_90_100.npy', X_90_100)\nnp.save('Y_90_100.npy', Y_90_100)\ndel X_90_100, Y_90_100\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-18T17:26:02.482324Z","iopub.execute_input":"2022-03-18T17:26:02.484519Z","iopub.status.idle":"2022-03-18T17:26:14.282027Z","shell.execute_reply.started":"2022-03-18T17:26:02.484459Z","shell.execute_reply":"2022-03-18T17:26:14.281366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## for now, just use a sub-sample to do some exploration\n#df = df.sample(frac=0.05, random_state=26)\n\n# sample using investment_id\n#sampled_ids = pd.Series(df['investment_id'].unique()).sample(frac=0.35, random_state=26)\n#df = df.loc[(df['investment_id'].isin(sampled_ids)) & (df['time_id']<=val_time_cutoff)]","metadata":{"execution":{"iopub.status.busy":"2022-03-16T18:23:08.02375Z","iopub.execute_input":"2022-03-16T18:23:08.024029Z","iopub.status.idle":"2022-03-16T18:23:08.02749Z","shell.execute_reply.started":"2022-03-16T18:23:08.023992Z","shell.execute_reply":"2022-03-16T18:23:08.026843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ndf = df.loc[df['time_id']<=val_time_cutoff]\nX_train = df[['investment_id']+features].values\nY_train = df['target'].values\n\n# save train data to file\nnp.save('X_train.npy', X_train)\nnp.save('Y_train.npy', Y_train)\n\ndel X_train\ndel Y_train\ngc.collect()\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering and Validation Set","metadata":{}},{"cell_type":"code","source":"'''def _add_lag(df, cols, lag, groupCol):\n    new_cols = [col+'_last'+str(lag) for col in cols]\n    df[new_cols] = df.groupby(groupCol)[cols].fillna(method='ffill').transform(lambda x: x.rolling(window=lag).mean())\n    df[new_cols] = df.groupby(groupCol)[new_cols].shift(1)\n\n    return df, new_cols'''","metadata":{"execution":{"iopub.status.busy":"2022-02-28T16:46:16.641813Z","iopub.execute_input":"2022-02-28T16:46:16.642644Z","iopub.status.idle":"2022-02-28T16:46:16.652618Z","shell.execute_reply.started":"2022-02-28T16:46:16.642599Z","shell.execute_reply":"2022-02-28T16:46:16.651632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df = df.sort_values(by=['time_id','investment_id'])","metadata":{"execution":{"iopub.status.busy":"2022-02-28T16:46:16.658346Z","iopub.execute_input":"2022-02-28T16:46:16.65895Z","iopub.status.idle":"2022-02-28T16:46:16.663294Z","shell.execute_reply.started":"2022-02-28T16:46:16.658908Z","shell.execute_reply":"2022-02-28T16:46:16.662457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#features = ['f_'+str(i) for i in range(300)]","metadata":{"execution":{"iopub.status.busy":"2022-02-28T16:46:16.664853Z","iopub.execute_input":"2022-02-28T16:46:16.666201Z","iopub.status.idle":"2022-02-28T16:46:16.672238Z","shell.execute_reply.started":"2022-02-28T16:46:16.666157Z","shell.execute_reply":"2022-02-28T16:46:16.671299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''#df = df.drop(new_cols, 1) #if want to get a new lag and remove old lag columns\ndf = df.copy() #fixes gaps in index warning\ndf, new_cols = _add_lag(df, features + ['target'], 1, 'investment_id')\nfeatures += new_cols\n#features = new_cols\ndf = df.fillna(0.)'''","metadata":{"execution":{"iopub.status.busy":"2022-02-28T16:46:16.675385Z","iopub.execute_input":"2022-02-28T16:46:16.677591Z","iopub.status.idle":"2022-02-28T16:46:16.685949Z","shell.execute_reply.started":"2022-02-28T16:46:16.677547Z","shell.execute_reply":"2022-02-28T16:46:16.685222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initial strategy will be to hold out the highest ~25% of data in terms of time_id as a validation set","metadata":{}},{"cell_type":"code","source":"'''\nval_time_cutoff = np.floor(df['time_id'].quantile(0.75))\nprint (val_time_cutoff)\nX_train = df.loc[df['time_id']<=val_time_cutoff, features].values\nY_train = df.loc[df['time_id']<=val_time_cutoff, 'target'].values\nX_val = df.loc[df['time_id']>val_time_cutoff, features].values\nY_val = df.loc[df['time_id']>val_time_cutoff, 'target'].values'''","metadata":{"execution":{"iopub.status.busy":"2022-02-28T16:46:16.688218Z","iopub.execute_input":"2022-02-28T16:46:16.689738Z","iopub.status.idle":"2022-02-28T16:46:16.712875Z","shell.execute_reply.started":"2022-02-28T16:46:16.689697Z","shell.execute_reply":"2022-02-28T16:46:16.712191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T17:40:38.290381Z","iopub.execute_input":"2022-03-28T17:40:38.293282Z","iopub.status.idle":"2022-03-28T17:40:38.701007Z","shell.execute_reply.started":"2022-03-28T17:40:38.29324Z","shell.execute_reply":"2022-03-28T17:40:38.700292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom scipy.stats import pearsonr\nimport lightgbm as lgb\nimport random\n\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.models import Sequential, load_model, Model\nfrom keras.layers import Input, IntegerLookup, Embedding, Concatenate, Reshape\nfrom keras import backend as K\nimport keras\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2022-03-28T18:39:43.449898Z","iopub.execute_input":"2022-03-28T18:39:43.450305Z","iopub.status.idle":"2022-03-28T18:39:49.904011Z","shell.execute_reply.started":"2022-03-28T18:39:43.450262Z","shell.execute_reply":"2022-03-28T18:39:49.903214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=70, random_state=26)\n\nX_train = np.load('X_train.npy')\nX_train_pca = pca.fit_transform(X_train)\ndel X_train\n\nnp.save('X_train_pca.npy', X_train_pca)\ndel X_train_pca\n\nX_val = np.load('X_val.npy')\nX_val_pca = pca.transform(X_val)\ndel X_val\n\nnp.save('X_val_pca.npy', X_val_pca)\ndel X_val_pca\n\nwith open('pca.pkl', 'wb') as f:\n    pickle.dump(pca, f)\n    \ndel pca\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T20:22:42.659446Z","iopub.execute_input":"2022-02-11T20:22:42.659831Z","iopub.status.idle":"2022-02-11T20:24:10.034757Z","shell.execute_reply.started":"2022-02-11T20:22:42.659795Z","shell.execute_reply":"2022-02-11T20:24:10.033506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LinearRegression()\n\n#lr.fit(X_train, Y_train)\n#print ('Validation Pearson r: {}'.format(str(pearsonr(lr.predict(X_val), Y_val)[0])))\nX_train_pca = np.load('X_train_pca.npy')\nY_train = np.load('Y_train.npy')\nlr.fit(X_train_pca, Y_train)\n\nwith open('lr.pkl', 'wb') as f:\n    pickle.dump(lr, f)\n    \ndel X_train_pca\ndel Y_train\n\nX_val_pca = np.load('X_val_pca.npy')\nY_val = np.load('Y_val.npy')\nprint ('Validation Pearson r: {}'.format(str(pearsonr(lr.predict(X_val_pca), Y_val)[0])))\n\ndel X_val_pca\ndel Y_val\ndel lr\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T20:27:03.903781Z","iopub.execute_input":"2022-02-11T20:27:03.904744Z","iopub.status.idle":"2022-02-11T20:27:12.974837Z","shell.execute_reply.started":"2022-02-11T20:27:03.904692Z","shell.execute_reply":"2022-02-11T20:27:12.97394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'boosting_type': 'gbdt',\n    'metric': 'mse',\n    'objective': 'regression',\n    'n_jobs': -1,\n    'seed': 26\n}\n\nkfold = StratifiedKFold(3, shuffle=True, random_state=26)\nfold_scores = []\nmodel_i = 1\nfor train_index, val_index in kfold.split(np.zeros(len(investment_id.index)), investment_id):\n    X = np.load('X.npy')[:,1:]\n    Xt = X[train_index]\n    Xv = X[val_index]\n    del X\n    Y = np.load('Y.npy')\n    Yt = Y[train_index]\n    Yv = Y[val_index]\n    del Y\n    gc.collect()\n    \n    gb = lgb.train(params, lgb.Dataset(Xt, Yt))\n    fold_scores.append(pearsonr(gb.predict(Xv), Yv)[0])\n    print (fold_scores)\n    \n    with open('gb_{}.pkl'.format(str(model_i)), 'wb') as f:\n        pickle.dump(gb, f)\n    del gb, Xv, Xt, Yv, Yt\n    gc.collect()\n    model_i += 1\n\nprint ('Mean Pearson r: '+str(np.mean(fold_scores)))","metadata":{"execution":{"iopub.status.busy":"2022-03-02T14:57:17.308279Z","iopub.execute_input":"2022-03-02T14:57:17.309285Z","iopub.status.idle":"2022-03-02T15:18:16.379526Z","shell.execute_reply.started":"2022-03-02T14:57:17.30925Z","shell.execute_reply":"2022-03-02T15:18:16.378397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def correlation_coefficient_loss(y_true, y_pred):\n    x = y_true\n    y = y_pred\n    mx = K.mean(x)\n    my = K.mean(y)\n    xm, ym = x-mx, y-my\n    r_num = K.sum(tf.multiply(xm,ym))\n    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n    r = r_num / r_den\n\n    r = K.maximum(K.minimum(r, 1.0), -1.0)\n    return 1 - K.square(r)\n\ndef create_model_nn():\n    features_input = Input((len(features),))\n    \n    x = Dropout(0.2)(features_input)\n    x = Dense(4096, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(32, activation='relu')(x)\n    out = Dense(1)(x)\n    \n    model = keras.Model(inputs=[features_input], outputs=[out])\n    model.compile(loss=correlation_coefficient_loss, optimizer=tf.optimizers.Adam(0.001))\n    return model\n\ndef create_model_nn2():\n    features_input = Input((len(features),))\n    \n    x = Dense(2048, activation='gelu')(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dense(512, activation='gelu')(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dense(32, activation='relu')(x)\n    out = Dense(1)(x)\n    \n    model = keras.Model(inputs=[features_input], outputs=[out])\n    model.compile(loss=correlation_coefficient_loss, optimizer=tf.optimizers.Adam(0.001))\n    return model\n\nfeatures = ['f_'+str(i) for i in range(300)]\n\nkfold = StratifiedKFold(5, shuffle=True, random_state=26)\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n\nfold_scores = []\n\nmodel_i = 1\nfor train_index, val_index in kfold.split(np.zeros(len(investment_id.index)), investment_id):\n    X = np.load('X.npy')\n    Xt = X[train_index]\n    Xv = X[val_index]\n    del X\n    Y = np.load('Y.npy')\n    Yt = Y[train_index]\n    Yv = Y[val_index]\n    del Y\n    gc.collect()\n    \n    nn = create_model_nn()\n    nn.fit(Xt[:,1:], Yt, epochs=50, batch_size=1024, validation_data=(Xv[:,1:], Yv), callbacks=[early_stop])\n    fold_scores.append(pearsonr(nn.predict(Xv[:,1:])[:,0], Yv)[0])\n    nn.save('nn_{}.h5'.format(str(model_i)))\n    del nn\n    gc.collect()\n    \n    print (fold_scores)\n    \n    del Xv, Xt, Yv, Yt\n    gc.collect()\n    model_i += 1\n\nprint ('Mean Pearson r: '+str(np.mean(np.abs(fold_scores))))","metadata":{"execution":{"iopub.status.busy":"2022-03-04T19:44:36.32877Z","iopub.execute_input":"2022-03-04T19:44:36.329037Z","iopub.status.idle":"2022-03-04T19:58:59.202098Z","shell.execute_reply.started":"2022-03-04T19:44:36.329009Z","shell.execute_reply":"2022-03-04T19:58:59.201341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5-fold stratified CV:\n\n4096, 512, 32 (with all 0.2s) relu: 0.1864 val, LB 0.144\n\n2048 relu, 1024 gelu, 512 gelu, 128 gelu, 32 relu (no dropouts): 0.1799 val, LB 0.145","metadata":{}},{"cell_type":"code","source":"params = {\n    'boosting_type': 'gbdt',\n    'metric': 'mse',\n    'objective': 'regression',\n    'n_jobs': -1,\n    'seed': 26\n}\n\ndef correlation_coefficient_loss(y_true, y_pred):\n    x = y_true\n    y = y_pred\n    mx = K.mean(x)\n    my = K.mean(y)\n    xm, ym = x-mx, y-my\n    r_num = K.sum(tf.multiply(xm,ym))\n    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n    r = r_num / r_den\n\n    r = K.maximum(K.minimum(r, 1.0), -1.0)\n    return 1 - K.square(r)\n\ndef create_model_nn(ids):\n    id_input = Input((1,))\n    features_input = Input((len(features),))\n    \n    ids = np.unique(ids)\n    x_ids = IntegerLookup(max_tokens=len(ids)+1, vocabulary=ids)(id_input)\n    x_ids = Embedding(len(ids)+1, 64, input_length=1)(x_ids)\n    x_ids = Reshape((-1,))(x_ids)\n    x_ids = Dense(64, activation='swish')(x_ids)\n    x_ids = Dense(64, activation='swish')(x_ids)\n    x_ids = Dense(64, activation='swish')(x_ids)\n    \n    x_features = Dense(512, activation='swish')(features_input)\n    x_features = Dense(512, activation='swish')(x_features)\n    x_features = Dense(512, activation='swish')(x_features)\n    x_features = Dense(512, activation='swish')(x_features)\n    \n    concat = Concatenate(axis=1)([x_features, x_ids])\n    \n    x = Dropout(0.2)(concat)\n    x = Dense(512, activation='swish')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(128, activation='swish')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(32, activation='swish')(x)\n    out = Dense(1)(x)\n    \n    model = keras.Model(inputs=[id_input, features_input], outputs=[out])\n    model.compile(loss=correlation_coefficient_loss, optimizer=tf.optimizers.Adam(0.001))\n    return model\n\ndef create_model_nn2():\n    features_input = Input((len(features),))\n    \n    x = Dropout(0.2)(features_input)\n    x = Dense(2048, activation='swish')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(1024, activation='swish')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(512, activation='swish')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(128, activation='swish')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(32, activation='swish')(x)\n    out = Dense(1)(x)\n    \n    model = keras.Model(inputs=[features_input], outputs=[out])\n    model.compile(loss=correlation_coefficient_loss, optimizer=tf.optimizers.Adam(0.001))\n    return model\n\nfeatures = ['f_'+str(i) for i in range(300)]\n\nkfold = StratifiedKFold(5, shuffle=True, random_state=26)\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n\nfold_scores = []\n\nmodel_i = 1\nfor train_index, val_index in kfold.split(np.zeros(len(investment_id.index)), investment_id):\n    X = np.load('X.npy')\n    Xt = X[train_index]\n    Xv = X[val_index]\n    del X\n    Y = np.load('Y.npy')\n    Yt = Y[train_index]\n    Yv = Y[val_index]\n    del Y\n    gc.collect()\n    \n    nn = create_model_nn(Xt[:,0])\n    nn.fit([Xt[:,0], Xt[:,1:]], Yt, epochs=50, batch_size=1024, validation_data=([Xv[:,0], Xv[:,1:]], Yv), callbacks=[early_stop])\n    #fold_scores.append(pearsonr(nn.predict([Xv[:,0], Xv[:,1:]])[:,0], Yv)[0])\n    preds_nn = nn.predict([Xv[:,0], Xv[:,1:]])[:,0]\n    nn.save('nn_{}.h5'.format(str(model_i)))\n    del nn\n    pr_1 = pearsonr(preds_nn, Yv)[0]\n    if pr_1<0:\n        preds_nn *= -1\n    gc.collect()\n    \n    nn2 = create_model_nn2()\n    nn2.fit(Xt[:,1:], Yt, epochs=50, batch_size=1024, validation_data=(Xv[:,1:], Yv), callbacks=[early_stop])\n    #fold_scores.append(pearsonr(nn.predict(Xv[:,1:])[:,0], Yv)[0])\n    \n    #gb = lgb.train(params, lgb.Dataset(Xt[:,1:], Yt))\n    preds_nn2 = nn2.predict(Xv[:,1:])[:,0]\n    nn2.save('nn2_{}.h5'.format(str(model_i)))\n    del nn2\n    pr_2 = pearsonr(preds_nn2, Yv)[0]\n    if pr_2<0:\n        preds_nn2 *= -1\n    gc.collect()\n    preds_combined = (preds_nn + preds_nn2)/2.\n    fold_scores.append([pr_1, pr_2, pearsonr(preds_combined, Yv)[0]])\n    \n    print (fold_scores)\n    \n    del Xv, Xt, Yv, Yt\n    gc.collect()\n    model_i += 1\n\nprint ('Mean Pearson r: '+str(np.mean(np.abs(fold_scores))))","metadata":{"execution":{"iopub.status.busy":"2022-03-03T20:07:50.515989Z","iopub.execute_input":"2022-03-03T20:07:50.516289Z","iopub.status.idle":"2022-03-03T20:42:45.457604Z","shell.execute_reply.started":"2022-03-03T20:07:50.516256Z","shell.execute_reply":"2022-03-03T20:42:45.456635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scores\nVal scheme: 3 stratified folds on investment_id, mean pearson r across the folds\n\nstarting architecture: 0.1772, LB 0.137\n\nwith 1024 x_feature layers and dropout over regularizers: 0.1812/0.1743, LB 0.143\n\nKEEP dropout over regularizers: 0.1728\n\nKEEP with 4X 512 x_feature layers: 0.1815/0.1800/0.1813, LB 0.137\n\nnn2, 2048, 1024, 512, 128, 32 with 0.2 dropouts: 0.1826, LB 0.140\n\nlgbm (with investment_id): 0.1709, LB 0.132","metadata":{}},{"cell_type":"markdown","source":"# Ensembling","metadata":{}},{"cell_type":"code","source":"#del X_train_pca, X_train\n#gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T19:49:57.881075Z","iopub.execute_input":"2022-02-11T19:49:57.881696Z","iopub.status.idle":"2022-02-11T19:49:58.212448Z","shell.execute_reply.started":"2022-02-11T19:49:57.881646Z","shell.execute_reply":"2022-02-11T19:49:58.211326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('lr.pkl', 'rb') as f:\n    lr = pickle.load(f)\n    \nwith open('gb.pkl', 'rb') as f:\n    gb = pickle.load(f)\n    \nnn = load_model('nn.h5')","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:28:52.813957Z","iopub.execute_input":"2022-02-11T21:28:52.814378Z","iopub.status.idle":"2022-02-11T21:28:52.931968Z","shell.execute_reply.started":"2022-02-11T21:28:52.814341Z","shell.execute_reply":"2022-02-11T21:28:52.931263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try just mean of previous predictions\nX_val_pca = np.load('X_val_pca.npy')\npreds_lr = lr.predict(X_val_pca)\ndel X_val_pca\n\nX_val = np.load('X_val.npy')\npreds_gb = gb.predict(X_val)\npreds_nn = nn.predict(X_val)[:,0]\ndel X_val\n\npreds = (preds_lr + preds_gb + preds_nn)/3\n\nY_val = np.load('Y_val.npy')\nprint ('Validation Pearson r: {}'.format(str(pearsonr(preds, Y_val)[0])))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:28:53.450516Z","iopub.execute_input":"2022-02-11T21:28:53.451813Z","iopub.status.idle":"2022-02-11T21:29:43.848082Z","shell.execute_reply.started":"2022-02-11T21:28:53.451761Z","shell.execute_reply":"2022-02-11T21:29:43.844466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Train on Train+Val","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=70, random_state=26)\n\nX_train = np.load('X_train.npy')\nX_val = np.load('X_val.npy')\nX = np.concatenate([X_train, X_val])\ndel X_train, X_val\nnp.save('X.npy', X)\nX_pca = pca.fit_transform(X)\ndel X\n\nnp.save('X_pca.npy', X_pca)\n\nwith open('pca.pkl', 'wb') as f:\n    pickle.dump(pca, f)\n    \ndel pca\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T19:37:02.378491Z","iopub.execute_input":"2022-02-14T19:37:02.378833Z","iopub.status.idle":"2022-02-14T19:39:09.022131Z","shell.execute_reply.started":"2022-02-14T19:37:02.378795Z","shell.execute_reply":"2022-02-14T19:39:09.021143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LinearRegression()\n\nY_train = np.load('Y_train.npy')\nY_val = np.load('Y_val.npy')\nY = np.concatenate([Y_train, Y_val])\ndel Y_train, Y_val\nnp.save('Y.npy', Y)\nlr = lr.fit(X_pca, Y)\ndel X_pca\n\nwith open('lr.pkl', 'wb') as f:\n    pickle.dump(lr, f)\n    \ndel lr\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T19:39:52.505411Z","iopub.execute_input":"2022-02-14T19:39:52.505747Z","iopub.status.idle":"2022-02-14T19:40:03.381844Z","shell.execute_reply.started":"2022-02-14T19:39:52.505714Z","shell.execute_reply":"2022-02-14T19:40:03.380964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.load('X.npy')\nY = np.load('Y.npy')\ngb = lgb.train(params, lgb.Dataset(X[:,1:], Y))\ndel X, Y\n\nwith open('gb.pkl', 'wb') as f:\n    pickle.dump(gb, f)\n\ndel gb\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T16:33:51.422526Z","iopub.execute_input":"2022-03-02T16:33:51.422837Z","iopub.status.idle":"2022-03-02T16:44:03.495914Z","shell.execute_reply.started":"2022-03-02T16:33:51.422805Z","shell.execute_reply":"2022-03-02T16:44:03.494823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def correlation_coefficient_loss(y_true, y_pred):\n    x = y_true\n    y = y_pred\n    mx = K.mean(x)\n    my = K.mean(y)\n    xm, ym = x-mx, y-my\n    r_num = K.sum(tf.multiply(xm,ym))\n    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n    r = r_num / r_den\n\n    r = K.maximum(K.minimum(r, 1.0), -1.0)\n    return 1 - K.square(r)\n\ndef create_model_nn(ids):\n    id_input = Input((1,))\n    features_input = Input((len(features),))\n    \n    ids = np.unique(ids)\n    x_ids = IntegerLookup(max_tokens=len(ids)+1, vocabulary=ids)(id_input)\n    x_ids = Embedding(len(ids)+1, 64, input_length=1)(x_ids)\n    x_ids = Reshape((-1,))(x_ids)\n    x_ids = Dense(64, activation='swish')(x_ids)\n    x_ids = Dense(64, activation='swish')(x_ids)\n    x_ids = Dense(64, activation='swish')(x_ids)\n    \n    x_features = Dense(512, activation='swish')(features_input)\n    x_features = Dense(512, activation='swish')(x_features)\n    x_features = Dense(512, activation='swish')(x_features)\n    x_features = Dense(512, activation='swish')(x_features)\n    \n    concat = Concatenate(axis=1)([x_features, x_ids])\n    \n    x = Dropout(0.2)(concat)\n    x = Dense(512, activation='swish')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(128, activation='swish')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(32, activation='swish')(x)\n    out = Dense(1)(x)\n    \n    model = keras.Model(inputs=[id_input, features_input], outputs=[out])\n    model.compile(loss=correlation_coefficient_loss, optimizer=tf.optimizers.Adam(0.001))\n    return model\n\nfeatures = ['f_'+str(i) for i in range(300)]\nY = np.load('Y.npy')\n\nkfold = StratifiedKFold(5, shuffle=True, random_state=26)\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n\nfold_scores = []\n\nmodel_i = 1\nfor train_index, val_index in kfold.split(np.zeros(len(investment_id.index)), investment_id):\n    X = np.load('X.npy')\n    Xt = X[train_index]\n    Xv = X[val_index]\n    del X\n    Y = np.load('Y.npy')\n    Yt = Y[train_index]\n    Yv = Y[val_index]\n    del Y\n    gc.collect()\n    \n    nn = create_model_nn(Xt[:,0])\n    nn.fit([Xt[:,0], Xt[:,1:]], Yt, epochs=50, batch_size=1024, validation_data=([Xv[:,0], Xv[:,1:]], Yv), callbacks=[early_stop])\n    fold_scores.append(pearsonr(nn.predict([Xv[:,0], Xv[:,1:]])[:,0], Yv)[0])\n    print (fold_scores)\n    \n    nn.save('nn_{}.h5'.format(str(model_i)))\n    del nn\n    gc.collect()\n    model_i += 1\n\nprint ('Mean Pearson r: '+str(np.mean(np.abs(fold_scores))))\ndel Xv, Xt, Yv, Yt","metadata":{"execution":{"iopub.status.busy":"2022-03-01T20:46:06.646297Z","iopub.execute_input":"2022-03-01T20:46:06.646744Z","iopub.status.idle":"2022-03-01T21:00:24.440659Z","shell.execute_reply.started":"2022-03-01T20:46:06.646706Z","shell.execute_reply":"2022-03-01T21:00:24.439692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Tuple\n\nclass GroupTimeSeriesSplit:\n    \"\"\"\n    Custom class to create a Group Time Series Split. We ensure\n    that the time id values that are in the testing data are not a part\n    of the training data & the splits are temporal\n    \"\"\"\n    def __init__(self, n_folds: int, holdout_size: int, groups: str) -> None:\n        self.n_folds = n_folds\n        self.holdout_size = holdout_size\n        self.groups = groups\n\n    def split(self, X) -> Tuple[np.array, np.array]:\n        # Take the group column and get the unique values\n        unique_time_ids = np.unique(self.groups.values)\n\n        # Split the time ids into the length of the holdout size\n        # and reverse so we work backwards in time. Also, makes\n        # it easier to get the correct time_id values per\n        # split\n        array_split_time_ids = np.array_split(\n            unique_time_ids, len(unique_time_ids) // self.holdout_size\n        )[::-1]\n\n        # Get the first n_folds values\n        array_split_time_ids = array_split_time_ids[:self.n_folds]\n\n        for time_ids in array_split_time_ids:\n            # Get test index - time id values that are in the time_ids\n            test_condition = X['time_id'].isin(time_ids)\n            test_index = X.loc[test_condition].index\n\n            # Get train index - The train index will be the time\n            # id values right up until the minimum value in the test\n            # data - we can also add a gap to this step by\n            # time id < (min - gap)\n            train_condition = X['time_id'] < (np.min(time_ids))\n            train_index = X.loc[train_condition].index\n\n            yield train_index, test_index","metadata":{"execution":{"iopub.status.busy":"2022-03-28T18:39:54.513739Z","iopub.execute_input":"2022-03-28T18:39:54.514332Z","iopub.status.idle":"2022-03-28T18:39:54.523612Z","shell.execute_reply.started":"2022-03-28T18:39:54.51429Z","shell.execute_reply":"2022-03-28T18:39:54.522927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ['f_'+str(i) for i in range(300)]\n\ndef correlation_coefficient_loss(y_true, y_pred):\n    x = y_true\n    y = y_pred\n    mx = K.mean(x)\n    my = K.mean(y)\n    xm, ym = x-mx, y-my\n    r_num = K.sum(tf.multiply(xm,ym))\n    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n    r = r_num / r_den\n\n    r = K.maximum(K.minimum(r, 1.0), -1.0)\n    return 1 - K.square(r)\n\ndef create_model_nn():\n    features_input = Input((len(features),))\n    \n    #x = Dropout(0.2)(features_input)\n    x = Dense(2048, activation='relu')(features_input)\n    x = Dense(1024, activation='gelu')(x)\n    x = Dense(512, activation='gelu')(x)\n    #x = Dropout(0.2)(x)\n    x = Dense(128, activation='gelu')(x)\n    #x = Dropout(0.2)(x)\n    x = Dense(32, activation='relu')(x)\n    out = Dense(1)(x)\n    \n    model = keras.Model(inputs=[features_input], outputs=[out])\n    model.compile(loss=correlation_coefficient_loss, optimizer=tf.optimizers.Adam(0.001))\n    return model\n\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n#X = np.load('X.npy')\n#Y = np.load('Y.npy')\n\ndf = pd.read_parquet(\"../input/ubiquant-parquet/train_low_mem.parquet\")\ngtss = GroupTimeSeriesSplit(n_folds=3, holdout_size=200, groups=df['time_id'])\n#del X\nfold_scores = []\ni = 1\nfor fold, (train_index, val_index) in enumerate(gtss.split(df)):\n    if i==1:\n        del df\n    X = np.load('X.npy')\n    Xt = X[train_index]\n    Xv = X[val_index]\n    del X\n    Y = np.load('Y.npy')\n    Yt = Y[train_index]\n    Yv = Y[val_index]\n    del Y\n    gc.collect()\n    \n    nn = create_model_nn()\n    nn.fit(Xt[:,2:], Yt, epochs=50, batch_size=1024, validation_data=(Xv[:,2:], Yv), callbacks=[early_stop])\n    preds_nn = nn.predict(Xv[:,2:])[:,0]\n    nn.save('nn_{}.h5'.format(str(i)))\n    del nn\n    gc.collect()\n    \n    df_val = pd.DataFrame()\n    df_val['preds'] = preds_nn\n    df_val['time_id'] = Xv[:,1]\n    df_val['Y'] = Yv\n    \n    r = df_val.groupby('time_id').apply(lambda x: pearsonr(df_val['Y'], df_val['preds'])[0]).mean()\n    print (r)\n    fold_scores.append(r)\n    print (fold_scores)\n    gc.collect()\n    i += 1\n\nprint ('Mean Pearson r: '+str(np.mean(np.abs(fold_scores))))\ndel Xv, Xt, Yv, Yt\n\nnn = create_model_nn()\nX = np.load('X.npy')\nY = np.load('Y.npy')\nnn.fit(X[:,2:], Y, epochs=4, batch_size=1024)\nnn.save('nn.h5')\npreds_nn = nn.predict(X[1000000:,2:])[:,0]\ndel nn\ndf_val = pd.DataFrame()\ndf_val['preds'] = preds_nn\ndf_val['time_id'] = X[1000000:,1]\ndf_val['Y'] = Y[1000000:]\nr = df_val.groupby('time_id').apply(lambda x: pearsonr(df_val['Y'], df_val['preds'])[0]).mean()\nprint (r)\n\ndel X, Y, preds_nn, df_val\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T18:39:54.632523Z","iopub.execute_input":"2022-03-28T18:39:54.632882Z","iopub.status.idle":"2022-03-28T18:49:39.952057Z","shell.execute_reply.started":"2022-03-28T18:39:54.632851Z","shell.execute_reply":"2022-03-28T18:49:39.950139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3-fold group time fold CV (val score is mean of fold_scores only, not full train)\n\n4096, 512, 32 (with all 0.2s) relu: (-0.1342, 0.1272, -0.1194, -0.1790), mean 0.1269, LB 0.1389\n\n2048 relu, 1024 gelu, 512 gelu, 128 gelu, 32 relu (no dropouts):\n\noriginal architecture with id_lookup layer:\n\n5X 512, 32 (with all 0.2s) relu: (0.1399, -0.1297, 0.1245, 0.1764), mean 0.1314, LB 0.1375\n\n10X 256, 32 (with all 0.2s) relu: (-0.1321, -0.1253, -0.1170, -0.1536), mean 0.1248\n\n10X 256, 32 (with all 0.4s) relu: (0.1389, -0.1214, -0.1187, -0.1463), mean 0.1263, LB 0.1353","metadata":{}},{"cell_type":"code","source":"def correlation_coefficient_loss(y_true, y_pred):\n    x = y_true\n    y = y_pred\n    mx = K.mean(x)\n    my = K.mean(y)\n    xm, ym = x-mx, y-my\n    r_num = K.sum(tf.multiply(xm,ym))\n    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n    r = r_num / r_den\n\n    r = K.maximum(K.minimum(r, 1.0), -1.0)\n    return 1 - K.square(r)\n\ndef create_model_nn():\n    features_input = Input((len(features),))\n    \n    x = Dropout(0.4)(features_input)\n    #x = Dense(2048, activation='relu')(features_input)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(32, activation='relu')(x)\n    out = Dense(1)(x)\n    \n    model = keras.Model(inputs=[features_input], outputs=[out])\n    model.compile(loss=correlation_coefficient_loss, optimizer=tf.optimizers.Adam(0.001))\n    return model\n\ndef create_model_nn2(ids):\n    id_input = Input((1,))\n    features_input = Input((len(features),))\n    \n    ids = np.unique(ids)\n    x_ids = IntegerLookup(max_tokens=len(ids)+1, vocabulary=ids)(id_input)\n    x_ids = Embedding(len(ids)+1, 64, input_length=1)(x_ids)\n    x_ids = Reshape((-1,))(x_ids)\n    x_ids = Dense(64, activation='swish')(x_ids)\n    x_ids = Dense(64, activation='swish')(x_ids)\n    x_ids = Dense(64, activation='swish')(x_ids)\n    \n    x_features = Dense(256, activation='swish')(features_input)\n    x_features = Dense(256, activation='swish')(x_features)\n    x_features = Dense(256, activation='swish')(x_features)\n    \n    concat = Concatenate(axis=1)([x_features, x_ids])\n    \n    x = Dense(512, activation='swish', kernel_regularizer='l2')(concat)\n    x = Dense(128, activation='swish', kernel_regularizer='l2')(x)\n    x = Dense(32, activation='swish', kernel_regularizer='l2')(x)\n    out = Dense(1)(x)\n    \n    model = keras.Model(inputs=[id_input, features_input], outputs=[out])\n    model.compile(loss=correlation_coefficient_loss, optimizer=tf.optimizers.Adam(0.001))\n    return model\n\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n\n#filenames = ['50_60','60_70','70_80','80_90','90_100']\nfilenames = ['70_80','80_90','90_100']\n\nfold_scores = []\nfor i in range(3):\n    if i==0:\n        #X = np.load('X_0_50.npy')[:,1:]\n        X = np.load('X_0_50.npy')\n        Y = np.load('Y_0_50.npy')\n        #X = np.concatenate([X, np.load('X_50_60.npy')[:,1:]])\n        X = np.concatenate([X, np.load('X_50_60.npy')])\n        Y = np.concatenate([Y, np.load('Y_50_60.npy')])\n        #X = np.concatenate([X, np.load('X_60_70.npy')[:,1:]])\n        X = np.concatenate([X, np.load('X_60_70.npy')])\n        Y = np.concatenate([Y, np.load('Y_60_70.npy')])\n        \n    #Xv = np.load('X_{}.npy'.format(filenames[i]))[:,1:]\n    Xv = np.load('X_{}.npy'.format(filenames[i]))\n    Yv = np.load('Y_{}.npy'.format(filenames[i]))\n    \n    #nn = create_model_nn()\n    #nn.fit(X[:,2:], Y, epochs=50, batch_size=1024, validation_data=(Xv[:,2:], Yv), callbacks=[early_stop])\n    #preds_nn = nn.predict(Xv[:,2:])[:,0]\n    #nn.save('nn_{}.h5'.format(str(i+1)))\n    #del nn\n    \n    nn = create_model_nn2(X[:,0])\n    nn.fit([X[:,0], X[:,2:]], Y, epochs=50, batch_size=1024, validation_data=([Xv[:,0], Xv[:,2:]], Yv), callbacks=[early_stop])\n    preds_nn = nn.predict([Xv[:,0], Xv[:,2:]])[:,0]\n    nn.save('nn_{}.h5'.format(str(i+1)))\n    del nn\n    \n    gc.collect()\n    \n    df_val = pd.DataFrame()\n    df_val['preds'] = preds_nn\n    df_val['time_id'] = Xv[:,1]\n    df_val['Y'] = Yv\n    \n    #fold_scores.append(pearsonr(nn.predict(Xv)[:,0], Yv)[0])\n    r = df_val.groupby('time_id').apply(lambda x: pearsonr(df_val['Y'], df_val['preds'])[0]).mean()\n    print (r)\n    #print (pearsonr(preds_nn2, Yv)[0])\n    #if pearsonr(preds_nn, Yv)[0]<0:\n    #    preds_nn *= -1\n    #if pearsonr(preds_nn2, Yv)[0]<0:\n    #    preds_nn2 *= -1\n    #fold_scores.append(pearsonr((preds_nn + preds_nn2)/2., Yv)[0])\n    fold_scores.append(r)\n    print (fold_scores)\n    \n    X = np.concatenate([X, Xv])\n    Y = np.concatenate([Y, Yv])\n    del Xv, Yv, preds_nn, df_val\n    gc.collect()\n\nprint (X.shape)\nprint (np.mean(np.abs(fold_scores)))\n\n#nn = create_model_nn()\n#nn.fit(X[:,2:], Y, epochs=4, batch_size=1024)\n#nn.save('nn.h5')\n#preds_nn = nn.predict(X[1000000:,2:])[:,0]\nnn = create_model_nn2(X[:,0])\nnn.fit([X[:,0], X[:,2:]], Y, epochs=4, batch_size=1024)\nnn.save('nn.h5')\npreds_nn = nn.predict([X[1000000:,0], X[1000000:,2:]])[:,0]\ndel nn\ndf_val = pd.DataFrame()\ndf_val['preds'] = preds_nn\ndf_val['time_id'] = X[1000000:,1]\ndf_val['Y'] = Y[1000000:]\nr = df_val.groupby('time_id').apply(lambda x: pearsonr(df_val['Y'], df_val['preds'])[0]).mean()\nprint (r)\n\n#nn2 = create_model_nn2(X[:,0])\n#nn2.fit([X[:,0], X[:,1:]], Y, epochs=4, batch_size=1024)\n#nn2.save('nn.h5')\n#print (pearsonr(nn2.predict([X[1000000:,0], X[1000000:,1:]])[:,0], Y[1000000:])[0])\n\ndel X, Y, preds_nn, df_val#, nn2\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-18T18:00:33.13958Z","iopub.execute_input":"2022-03-18T18:00:33.139904Z","iopub.status.idle":"2022-03-18T18:09:08.347283Z","shell.execute_reply.started":"2022-03-18T18:00:33.139867Z","shell.execute_reply":"2022-03-18T18:09:08.346343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3-fold time split CV (val score is mean of fold_scores only, not full train)\n\n4096, 512, 32 (with all 0.2s) relu: (-0.1320, 0.1339, 0.1345, 0.2165) val, LB 0.139\n\n2048 relu, 1024 gelu, 512 gelu, 128 gelu, 32 relu (no dropouts):\n\noriginal architecture with id_lookup layer: (-0.1336, 0.1406, 0.1456, 0.2296), LB 0.137\n\n5X 512, 32 (with all 0.2s) relu:\n\n10X 256, 32 (with all 0.2s) relu: (0.1190, -0.1496, 0.1491, -0.1630) val, LB 0.140\n\n10X 256, 32 (with all 0.4s) relu: ","metadata":{}}]}